1. [Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)
- problem: overfiting, incapable of assessing the uncertainty in the training data
- solution: Bayes by Backprop
- results: Bayes by Backprop attains performance comparable
to that of dropout

Where does uncertainty come from?
- Aleatoric: A lot of this uncertainty comes from imperfect training data, which does not exactly describe the distribution the data were drawn from.
Aleatoric uncertainty is uncertainty in the data itself, for example, this could come from measurements error or noise in the labels of a dataset or it could come down to inherent stochasticity in the data-generating process itself. This kind of uncertainty is irreducible in the sense they represents a randomness in the problem that's always going to be there, regardless of the model you use or the amount of data you collect, we simply have to account for it. For instance, if you think of a simple coin flipping experiment, let's say we have a dataset of the outcomes of a large number of coin tosses, no model is going to be able to predict with a certainty what the outcome of the next coin toss is going to be. All we can do is estimate the probability of the coin turning up heads.  Aleatoric uncertainty comes in two flavors, homoscedastic and heteroscedastic. The distinction comes down to whether or not the noise is dependent on the input variable. If the data uncertainty is the same for all target variables, regardless of the input, then it is homoscedastic. If the data uncertainty varies according the input variable, then it's heteroscedastic
- epistemic. The other category of uncertainty is epistemic uncertainty which is model uncertainty. We can design a deep-learning model for a problem. That model might well have the capacity to model the underlying data distribution accurately, but if there isn't enough data available, then the model won't be able to learn effectively. In this case, the problem isn't down to randomness in the data, but rather to the uncertainty about which model parameters accurately model the data. In a small data regime, there are potentially many values of the parameters that could explain the data. Epistemic uncertainty is something that will decrease as we gather more data, since the model gets more information about which parameters accurately explain the data
